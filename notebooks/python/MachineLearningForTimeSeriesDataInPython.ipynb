{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c1e5fe-13db-4639-8c5c-77f441d72c54",
   "metadata": {},
   "source": [
    "# Chapter 1: Time Series and Machine Learning Primer\n",
    "The Kaggle Heartbeat data set is referenced.  I believe this can be found at \n",
    "[heartbeat sounds](https://www.kaggle.com/datasets/kinguistics/heartbeat-sounds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0020323-0266-496d-aed7-dc74163f5a02",
   "metadata": {},
   "source": [
    "## Time series kinds and applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da3436-f3e7-4819-96b1-0368d49d50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a pandas timeseries\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "data.plot('date', 'close', ax=ax)\n",
    "ax.set(title=\"AAPL daily closing price\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0058b28-f2e5-4d5f-a5ba-4d55ba0e003e",
   "metadata": {},
   "source": [
    "## Machine Learning Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a0000-16ad-4fbd-b529-653df391dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "# model = LinearSVC()\n",
    "# model.fit(X, y)\n",
    "# model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3116c91-3700-437a-bc07-d0a4fd454937",
   "metadata": {},
   "source": [
    "## Combining time series data with machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7235c2-4b93-45df-a12b-bef4ec48a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading auditory data\n",
    "from glob import glob\n",
    "files glob('data/heartbeat-sounds/files/*.wav')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162928c-1f31-4349-b576-05b3f2864585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in auditory data\n",
    "import librosa as lr\n",
    "#load accepts a path to an audio file\n",
    "audio, sfreq = lr.load('data/heartbeat-sounds/proc/files/murmur__201101051104.wav')\n",
    "print(sfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d237cbb-b25c-4fdc-a71e-a00b778030c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(0, len(audio))\n",
    "time = indices / sfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a137c-9146-4fcf-bf09-fec359367156",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time = (len(audio) 1) / sfreq\n",
    "time = np.linspace(0, final_time, sfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f27005-4bb2-4e9b-81dd-bf97c4eaee2b",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b393b9-f1b1-468a-8d3c-98bb4d645c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lr\n",
    "from glob import glob\n",
    "\n",
    "# List all the wav files in the folder\n",
    "audio_files = glob(data_dir + '/*.wav')\n",
    "\n",
    "# Read in the first audio file, create the time array\n",
    "audio, sfreq = lr.load(audio_files[0])\n",
    "time = np.arange(0, len(audio)) / sfreq\n",
    "\n",
    "# Plot audio over time\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(time, audio)\n",
    "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52487452-3cdf-494d-83df-49a3ec574057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('prices.csv', index_col=0)\n",
    "\n",
    "# Convert the index of the DataFrame to datetime\n",
    "data.index = pd.to_datetime(data.index)\n",
    "print(data.head())\n",
    "\n",
    "# Loop through each column, plot its values over time\n",
    "fig, ax = plt.subplots()\n",
    "for column in data:\n",
    "    data[column].plot(ax=ax, label=column)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe11a32-652b-4315-977d-48826a931ba7",
   "metadata": {},
   "source": [
    "# Chapter 2: Time Series as Inputs to a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7e913-b7b7-4e4a-92fd-690ddb2e3baf",
   "metadata": {},
   "source": [
    "## Classifying a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e7e68-f559-46a7-802b-7e53575aca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize your timeseries data!\n",
    "ixs = np.arange(audio.shape[-1])\n",
    "time = ixs / sfreq\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(time, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebe965-16fb-43d9-ab70-83c16d4ac315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating multiple features\n",
    "print(audio.shape)\n",
    "# (n_files, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df400a54-d291-450c-96b4-17c90c59e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(audio, axis=-1)\n",
    "maxs = np.max(audio, axis=-1)\n",
    "stds = np.std(audio, axis=-1)\n",
    "print(means.shape)\n",
    "# (n_files,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a54c87-ea6e-460d-840d-88bea8549f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing your features for scikit-learn\n",
    "# Import a linear classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "# Note that means are reshaped to work with scikit-learn\n",
    "X = np.column_stack([means, maxs, stds])\n",
    "y = labels.reshape(-1, 1)\n",
    "model = LinearSVC()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd1209-7060-4da0-98f8-470ab00ab640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring your scikit-learn model\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Different input data\n",
    "predictions = model.predict(X_test)\n",
    "# Score our model with % correct\n",
    "# Manually\n",
    "percent_score = sum(predictions == labels_test) / len(labels_test)\n",
    "# Using a sklearn scorer\n",
    "percent_score = accuracy_score(labels_test, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a2c7f-9c1c-4dd0-b1ae-01094af84390",
   "metadata": {},
   "source": [
    "## Improving features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742b308-44fa-4293-ae2d-a9b2e74a8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating a rolling window statistic\n",
    "# Audio is a Pandas DataFrame\n",
    "print(audio.shape)\n",
    "# (n_times, n_audio_files)\n",
    "# (5000, 20)\n",
    "\n",
    "# Smooth our data by taking the rolling mean in a window of 50 samples\n",
    "window_size = 50\n",
    "windowed = audio.rolling(window=window_size)\n",
    "audio_smooth = windowed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba46dda-e987-40a2-8f63-27cabaddf6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the auditory envelope\n",
    "# First rectify your audio, then smooth it\n",
    "audio_rectified = audio.apply(np.abs)\n",
    "audio_envelope = audio_rectified.rolling(50).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70a0fe-8799-4bb4-ae81-6d8964f55b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering the envelope\n",
    "# Calculate several features of the envelope, one per sound\n",
    "envelope_mean = np.mean(audio_envelope, axis=0)\n",
    "envelope_std = np.std(audio_envelope, axis=0)\n",
    "envelope_max = np.max(audio_envelope, axis=0)\n",
    "# Create our training data for a classifier\n",
    "X = np.column_stack([envelope_mean, envelope_std, envelope_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2a6f6-27e9-46f4-b586-0371cdf79baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our features for scikit-learn\n",
    "X = np.column_stack([envelope_mean, envelope_std, envelope_max])\n",
    "y = labels.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c973c-1863-41f7-a512-164350c8ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for classification\n",
    "# cross_val_score automates the process of:\n",
    "# Using cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LinearSVC()\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5a593-9204-479f-b59a-3a43c9702e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the tempogram\n",
    "# Import librosa and calculate the tempo of a 1-D sound array\n",
    "import librosa as lr\n",
    "audio_tempo = lr.beat.tempo(audio, sr=sfreq, hop_length=2**6, aggregate=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc19a88-bbd6-404b-ab67-a8222a0f08e3",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a3298-0807-4a9a-9394-3ed82ca92cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tempo of the sounds\n",
    "tempos = []\n",
    "for col, i_audio in audio.items():\n",
    "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
    "\n",
    "# Convert the list to an array so you can manipulate it more easily\n",
    "tempos = np.array(tempos)\n",
    "\n",
    "# Calculate statistics of each tempo\n",
    "tempos_mean = tempos.mean(axis=-1)\n",
    "tempos_std = tempos.std(axis=-1)\n",
    "tempos_max = tempos.max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40a575-7771-4fc7-a99e-6a904d21104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
    "y = labels.reshape(-1, 1)\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b6514-344a-43e5-b922-9447b717d2cb",
   "metadata": {},
   "source": [
    "## The spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793134f1-9eb8-4c62-914e-fb3cf2d65bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the STFT with code\n",
    "# Import the functions we'll use for the STFT\n",
    "from librosa.core import stft, amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate our STFT\n",
    "HOP_LENGTH = 2**4\n",
    "SIZE_WINDOW = 2**7\n",
    "audio_spec = stft(audio, hop_length=HOP_LENGTH, n_fft=SIZE_WINDOW)\n",
    "\n",
    "# Convert into decibels for visualization\n",
    "spec_db = amplitude_to_db(audio_spec)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots()\n",
    "specshow(spec_db, sr=sfreq, x_axis='time',y_axis='hz', ... # There should be more here, see call in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac022ab-068a-4fc5-9580-61bc76e761e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating spectral features\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
    "\n",
    "# Display these features on top of the spectrogram\n",
    "fig, ax = plt.subplots()\n",
    "specshow(spec, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH, ax=ax)\n",
    "ax.plot(times_spec, centroids)\n",
    "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40414ea-532d-48c9-9c5f-1f7f036ea83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining spectral and temporal features in a\n",
    "classifier\n",
    "centroids_all = []\n",
    "bandwidths_all = []\n",
    "for spec in spectrograms:\n",
    "    bandwidths = lr.feature.spectral_bandwidth(S=lr.db_to_amplitude(spec))\n",
    "    centroids = lr.feature.spectral_centroid(S=lr.db_to_amplitude(spec))\n",
    "    # Calculate the mean spectral bandwidth\n",
    "    bandwidths_all.append(np.mean(bandwidths))\n",
    "    # Calculate the mean spectral centroid\n",
    "    centroids_all.append(np.mean(centroids))\n",
    "\n",
    "# Create our X matrix\n",
    "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths_all, centroids_all])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a399318-f0b3-42ae-bce9-ecc2893c9134",
   "metadata": {},
   "source": [
    "# Chapter 3: Predicting Time Series Data\n",
    "**NOTE**: No data is loaded in this chapter, and so the examples in this section will likely not run in their current form and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8d495-0197-4cec-925a-6464368f1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815a7ae-bc66-4ecb-86b0-cdb2412b0bf5",
   "metadata": {},
   "source": [
    "## Predicting data over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f161b8-b9e3-48e4-90ad-72ff7925c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing relationships between timeseries\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "# Make a line plot for each timeseries\n",
    "axs[0].plot(x, c='k', lw=3, alpha=.2)\n",
    "axs[0].plot(y)\n",
    "axs[0].set(xlabel='time', title='X values = time')\n",
    "# Encode time as color in a scatterplot\n",
    "axs[1].scatter(x_long, y_long, c=np.arange(len(x_long)), cmap='viridis')\n",
    "axs[1].set(xlabel='x', ylabel='y', title='Color = time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d18154-0cdc-47e9-bb13-480336c09ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression models with scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44942856-cf76-4607-b161-336d408a247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with scikit-learn\n",
    "alphas = [.1, 1e2, 1e3]\n",
    "ax.plot(y_test, color='k', alpha=.3, lw=3)\n",
    "for ii, alpha in enumerate(alphas):\n",
    "    y_predicted = Ridge(alpha=alpha).fit(X_train, y_train).predict(X_test)\n",
    "    ax.plot(y_predicted, c=cmap(ii / len(alphas)))\n",
    "ax.legend(['True values', 'Model 1', 'Model 2', 'Model 3'])\n",
    "ax.set(xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa40de-1754-424e-a04b-ca5db1d54c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with scikit-learn\n",
    "\n",
    "# R in scikit-learn\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_predicted, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a07f6-8255-48d6-a5fd-c36a4ff50a3f",
   "metadata": {},
   "source": [
    "## Cleaning and improving your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3faa026-0939-41f4-af10-cf7e1e4ec049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation in Pandas\n",
    "# Return a boolean that notes where missing values are\n",
    "missing = prices.isna()\n",
    "\n",
    "# Interpolate linearly within missing windows\n",
    "prices_interp = prices.interpolate('linear')\n",
    "\n",
    "# Plot the interpolated data in red and the data w/ missing values in black\n",
    "ax = prices_interp.plot(c='r')\n",
    "prices.plot(c='k', ax=ax, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a98214-20f6-4d13-8e4d-6a19fb63c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming to percent change with Pandas\n",
    "def percent_change(values):\n",
    "    \"\"\"Calculates the % change between the last value\n",
    "    and the mean of previous values\"\"\"\n",
    "    # Separate the last value and all previous values into variables\n",
    "    previous_values = values[:-1]\n",
    "    last_value = values[-1\n",
    "    \n",
    "    # Calculate the % difference between the last value\n",
    "    # and the mean of earlier values\n",
    "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
    "    return percent_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fa048-053c-401a-9215-c020154198d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying this to our data\n",
    "# Plot the raw data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax = prices.plot(ax=axs[0])\n",
    "                 \n",
    "# Calculate % change and plot\n",
    "ax = prices.rolling(window=20).aggregate(percent_change).plot(ax=axs[1])\n",
    "ax.legend_.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d3ac7-6810-4cb3-b88b-b5112f206d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a threshold on our data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for data, ax in zip([prices, prices_perc_change], axs):\n",
    "    # Calculate the mean / standard deviation for the data\n",
    "    this_mean = data.mean()\n",
    "    this_std = data.std()\n",
    "    \n",
    "    # Plot the data, with a window that is 3 standard deviations\n",
    "    # around the mean\n",
    "    data.plot(ax=ax)\n",
    "    ax.axhline(this_mean + this_std * 3, ls='--', c='r')\n",
    "    ax.axhline(this_mean - this_std * 3, ls='--', c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069ecf6-4979-4c9b-95d7-87500c1a047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing outliers using the threshold\n",
    "# Center the data so the mean is 0\n",
    "prices_outlier_centered = prices_outlier_perc - prices_outlier_perc.mean()\n",
    "\n",
    "# Calculate standard deviation\n",
    "std = prices_outlier_perc.std()\n",
    "\n",
    "# Use the absolute value of each datapoint\n",
    "# to make it easier to find outliers\n",
    "outliers = np.abs(prices_outlier_centered) > (std * 3)\n",
    "\n",
    "# Replace outliers with the median value\n",
    "# We'll use np.nanmean since there may be nans around the outliers\n",
    "prices_outlier_fixed = prices_outlier_centered.copy()\n",
    "prices_outlier_fixed[outliers] = np.nanmedian(prices_outlier_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5a299-2b55-44ff-81fd-bbcfbf4ce865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "prices_outlier_centered.plot(ax=axs[0])\n",
    "prices_outlier_fixed.plot(ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee5e53-77f7-4dc7-9190-9ed60156ec92",
   "metadata": {},
   "source": [
    "## Creating features over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587fc0b-5066-41fb-90a5-1e5abd73e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .aggregate for feature extraction\n",
    "# Visualize the raw data\n",
    "print(prices.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2469bf-0ced-4769-a6f8-be76881a0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a rolling window, then extract two features\n",
    "feats = prices.rolling(20).aggregate([np.std, np.max]).dropna()\n",
    "print(feats.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c9adb-81ee-4dde-b4e9-744fceea7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using partial() in Python\n",
    "# If we just take the mean, it returns a single value\n",
    "a = np.array([[0, 1, 2], [0, 1, 2], [0, 1, 2]])\n",
    "print(np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdb3ab-2c6d-4a9f-a634-11d477d0caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the partial function to initialize np.mean\n",
    "# with an axis parameter\n",
    "from functools import partial\n",
    "mean_over_first_axis = partial(np.mean, axis=0)\n",
    "print(mean_over_first_axis(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d13c13-a1bc-459a-ba91-278c5ba83660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining np.percentile() with partial functions to\n",
    "# calculate a range of percentiles\n",
    "data = np.linspace(0, 100)\n",
    "# Create a list of functions using a list comprehension\n",
    "percentile_funcs = [partial(np.percentile, q=ii) for ii in [20, 40, 60]]\n",
    "# Calculate the output of each function in the same way\n",
    "percentiles = [i_func(data) for i_func in percentile_funcs]\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a611b3-35df-4690-980d-ea03a86cafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate multiple percentiles of a rolling window\n",
    "data.rolling(20).aggregate(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e8bf3-112c-4ce6-8a26-fd915b50576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime features using Pandas\n",
    "# Ensure our index is datetime\n",
    "prices.index = pd.to_datetime(prices.index)\n",
    "\n",
    "# Extract datetime features\n",
    "day_of_week_num = prices.index.weekday\n",
    "print(day_of_week_num[:10])\n",
    "\n",
    "day_of_week = prices.index.weekday_name\n",
    "print(day_of_week[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911f4d4-97ae-4e1b-8fea-c177886e4919",
   "metadata": {},
   "source": [
    "# Chapter 4: Validating and Inspecting Time Series Models\n",
    "**NOTE**: No data is loaded in this chapter, and so the examples in this section will likely not run in their current form and order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b11293-0ab7-4310-9132-abce75c05797",
   "metadata": {},
   "source": [
    "## Creating features from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfcc17-542c-412e-9eac-63f43722ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time-shifting data with Pandas\n",
    "print(df)\n",
    "\n",
    "# Shift a DataFrame/Series by 3 index values towards the past\n",
    "print(df.shift(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727271f-c3fa-48bd-a640-9a5df7d42bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a time-shifted DataFrame\n",
    "# data is a pandas Series containing time series data\n",
    "data = pd.Series(...)\n",
    "\n",
    "# Shifts\n",
    "shifts = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Create a dictionary of time-shifted data\n",
    "many_shifts = {'lag_{}'.format(ii): data.shift(ii) for ii in shifts}\n",
    "\n",
    "# Convert them into a dataframe\n",
    "many_shifts = pd.DataFrame(many_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e9e1b-0fe9-4d20-8d5a-c248c9d8d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a model with time-shifted features\n",
    "# Fit the model using these input features\n",
    "model = Ridge()\n",
    "model.fit(many_shifts, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790e76c-cf59-43a0-a09b-858adc2880d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting the auto-regressive model coefficients\n",
    "# Visualize the fit model coefficients\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(many_shifts.columns, model.coef_)\n",
    "ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
    "# Set formatting so it looks nice\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d86439-bc33-4c11-bfc9-2eb8c37f551e",
   "metadata": {},
   "source": [
    "## Cross-validating time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47971b-1c85-468c-a2d2-4ebf86470a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation with scikit-learn\n",
    "# Iterating over the \"split\" method yields train/test indices\n",
    "for tr, tt in cv.split(X, y):\n",
    "    model.fit(X[tr], y[tr])\n",
    "    model.score(X[tt], y[tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ced18-a7c7-4f6c-b054-4d4c2b3c5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation types: KFold\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5)\n",
    "for tr, tt in cv.split(X, y):\n",
    "    pass # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dbda46-00d8-49f8-b13e-fdd97d638555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing model predictions\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "# Plot the indices chosen for validation on each loop\n",
    "axs[0].scatter(tt, [0] * len(tt), marker='_', s=2, lw=40)\n",
    "axs[0].set(ylim=[-.1, .1], title='Test set indices (color=CV loop)',\n",
    "xlabel='Index of raw data')\n",
    "# Plot the model predictions on each iteration\n",
    "axs[1].plot(model.predict(X[tt]))\n",
    "axs[1].set(title='Test set predictions on each CV loop',\n",
    "xlabel='Prediction index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17f350-2098-4698-84a4-9c6f116424de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=3)\n",
    "for tr, tt in cv.split(X, y):\n",
    "    pass # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d7434-23b6-4e1e-a631-2211dc036b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing time series cross validation iterators\n",
    "# Import and initialize the cross-validation iterator\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Plot training and test indices\n",
    "    l1 = ax.scatter(tr, [ii] * len(tr), c=[plt.cm.coolwarm(.1)], marker='_', lw=6)\n",
    "    l2 = ax.scatter(tt, [ii] * len(tt), c=[plt.cm.coolwarm(.9)], marker='_', lw=6)\n",
    "    ax.set(ylim=[10, -1], title='TimeSeriesSplit behavior', xlabel='data index', ylabel='CV iteration')\n",
    "    ax.legend([l1, l2], ['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2dfa7-d999-4114-affb-e5a2b62dfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom scoring functions in scikit-learn\n",
    "def myfunction(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    my_custom_score = my_custom_function(y_pred, y)\n",
    "    return my_custom_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd278fdd-862b-4c84-850a-9469c126db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A custom correlation function for scikit-learn\n",
    "def my_pearsonr(est, X, y):\n",
    "    # Generate predictions and convert to a vector\n",
    "    y_pred = est.predict(X).squeeze()\n",
    "    # Use the numpy \"corrcoef\" function to calculate a correlation matrix\n",
    "    my_corrcoef_matrix = np.corrcoef(y_pred, y.squeeze())\n",
    "    # Return a single correlation value from the matrix\n",
    "    my_corrcoef = my_corrcoef_matrix[1, 0] # NOTE (JS): Fixed typo\n",
    "    return my_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721d3aa-6028-4b9c-9688-a6af27ea17c0",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cc847-5fa8-454a-9d4f-583c39e07363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ShuffleSplit and create the cross-validation object\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(10, random_state=1)\n",
    "\n",
    "# Iterate through CV splits\n",
    "results = []\n",
    "for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "    model.fit(X[tr], y[tr])\n",
    "    \n",
    "    # Generate predictions on the test data, score the predictions, and collect\n",
    "    prediction = model.predict(X[tt])\n",
    "    score = r2_score(y[tt], prediction)\n",
    "    results.append((prediction, score, tt))\n",
    "\n",
    "# Custom function to quickly visualize predictions\n",
    "visualize_predictions(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242bc29-e933-4d19-b468-95a5a350f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Iterate through CV splits\n",
    "fig, ax = plt.subplots()\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "    ax.plot(tr, ii + y[tr])\n",
    "\n",
    "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017bdc6-7cc3-48f9-af00-f009b3281c59",
   "metadata": {},
   "source": [
    "## Stationarity and stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cb081-013c-4b85-bd05-a924abef3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stationarity and stability\n",
    "# Bootstrapping the mean\n",
    "from sklearn.utils import resample\n",
    "# cv_coefficients has shape (n_cv_folds, n_coefficients)\n",
    "n_boots = 100\n",
    "bootstrap_means = np.zeros(n_boots, n_coefficients)\n",
    "for ii in range(n_boots):\n",
    "    # Generate random indices for our data with replacement,\n",
    "    # then take the sample mean\n",
    "    random_sample = resample(cv_coefficients)\n",
    "    bootstrap_means[ii] = random_sample.mean(axis=0)\n",
    "\n",
    "# Compute the percentiles of choice for the bootstrapped means\n",
    "percentiles = np.percentile(bootstrap_means, (2.5, 97.5), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16bb11-887e-4b31-bdc8-8c7080c5cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bootstrapped coefficients\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(many_shifts.columns, percentiles[0], marker='_', s=200)\n",
    "ax.scatter(many_shifts.columns, percentiles[1], marker='_', s=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b7be4-c174-45b9-baf2-765691b12820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance over time\n",
    "def my_corrcoef(est, X, y):\n",
    "    \"\"\"Return the correlation coefficient\n",
    "    between model predictions and a validation set.\"\"\"\n",
    "    return np.corrcoef(y, est.predict(X))[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f45ab-ac5a-4022-b958-2694a8573319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the date of the first index of each validation set\n",
    "first_indices = [data.index[tt[0]] for tr, tt in cv.split(X, y)]\n",
    "# Calculate the CV scores and convert to a Pandas Series\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring=my_corrcoef)\n",
    "cv_scores = pd.Series(cv_scores, index=first_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23524250-80dc-4858-a54f-cf6bceef3486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing model scores as a timeseries\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "# Calculate a rolling mean of scores over time\n",
    "cv_scores_mean = cv_scores.rolling(10, min_periods=1).mean()\n",
    "cv_scores.plot(ax=axs[0])\n",
    "axs[0].set(title='Validation scores (correlation)', ylim=[0, 1])\n",
    "# Plot the raw data\n",
    "data.plot(ax=axs[1])\n",
    "axs[1].set(title='Validation data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ca8cc-8dc9-4ad7-8860-5bc2900c0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed windows with time series cross-validation\n",
    "# Only keep the last 100 datapoints in the training data\n",
    "window = 100\n",
    "# Initialize the CV with this window size\n",
    "cv = TimeSeriesSplit(n_splits=10, max_train_size=window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c282600-fd4d-4bf0-bb75-799d93ba7455",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ceb040-ad01-487d-92d9-517b23798d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
    "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
    "    # Create our empty array to fill the results\n",
    "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
    "    for ii in range(n_boots):\n",
    "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
    "        random_sample = resample(data)\n",
    "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
    "        \n",
    "    # Compute the percentiles of choice for the bootstrapped means\n",
    "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
    "    return percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7ce90-5455-4970-a3f7-6cbd9d705820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a confidence interval around each coefficient\n",
    "bootstrapped_interval = bootstrap_interval(coefficients)\n",
    "\n",
    "# Plot it\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
    "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
    "ax.set(title='95% confidence interval for model coefficients')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c72c87-2bba-445c-93d6-801d6e98ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a confidence interval around each coefficient\n",
    "bootstrapped_interval = bootstrap_interval(coefficients)\n",
    "\n",
    "# Plot it\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
    "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
    "ax.set(title='95% confidence interval for model coefficients')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34b347-adea-45e8-a2a7-c6cbfcc96c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "scores_lo.plot(ax=ax, label=\"Lower confidence interval\")\n",
    "scores_hi.plot(ax=ax, label=\"Upper confidence interval\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85f825-1e32-401e-abd9-41ad6fa26d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-initialize window sizes\n",
    "window_sizes = [25, 50, 75, 100]\n",
    "\n",
    "# Create an empty DataFrame to collect the stores\n",
    "all_scores = pd.DataFrame(index=times_scores)\n",
    "\n",
    "# Generate scores for each split to see how the model performs over time\n",
    "for window in window_sizes:\n",
    "    # Create cross-validation object using a limited lookback window\n",
    "    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n",
    "    \n",
    "    # Calculate scores across all CV splits and collect them in a DataFrame\n",
    "    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "    all_scores['Length {}'.format(window)] = this_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a778d-a8dd-4570-838e-e60948b6126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the scores\n",
    "ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm)\n",
    "ax.set(title='Scores for multiple windows', ylabel='Correlation (r)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
