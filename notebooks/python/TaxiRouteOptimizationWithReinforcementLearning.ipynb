{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe593719-b630-46ef-9239-43b03910ba95",
   "metadata": {},
   "source": [
    "# The Taxi-v3 environment\n",
    "The Taxi-v3 environment is a strategic simulation, offering a grid-based arena where a taxi navigates to address daily challenges akin to those faced by a taxi driver. This environment is defined by a 5x5 grid where the taxi's mission involves picking up a passenger from one of four specific locations (marked as Red, Green, Yellow, and Blue) and dropping them off at another designated spot. The goal is to accomplish this with minimal time on the road to maximize rewards, emphasizing the need for route optimization and efficient decision-making for passenger pickup and dropoff.\n",
    "\n",
    "## Key Components:\n",
    "* Action Space: Comprises six actions where 0 moves the taxi south, 1 north, 2 east, 3 west, 4 picks up a passenger, and 5 drops off a passenger.\n",
    "* Observation Space: Comprises 500 discrete states, accounting for 25 taxi positions, 5 potential passenger locations, and 4 destinations.\n",
    "* Rewards System: Includes a penalty of -1 for each step taken without other rewards, +20 for successful passenger delivery, and -10 for illegal pickup or dropoff actions. Actions resulting in no operation, like hitting a wall, also incur a time step penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65967847-993a-4852-a000-e2b87d114c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell to install and import the necessary libraries and load the required variables\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "# Initialize the Taxi-v3 environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n",
    "\n",
    "# Seed the environment for reproducibility\n",
    "env.np_random, _ = seeding.np_random(42)\n",
    "env.action_space.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Maximum number of actions per training episode\n",
    "max_actions = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89abfe4-c7e9-433a-aebc-c33dcea2fd3d",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* Train an agent over 2,000 episodes, allowing for a maximum of 100 actions per episode (`max_actions`), utilizing Q-learning.  Record the total rewards achieved in each episode and save these in a list named `episode_returns`.\n",
    "* What are the learned Q-values? Save these in a numpy array named `q_table`.\n",
    "* What is the learned policy? Save it in a dictionary named `policy`.\n",
    "* Test the agent's learned policy for one episode, starting with a seed of 42. Save the encountered states from `env.render()` as frames in a list named `frames`, and the sum of collected rewards in a variable named `episode_total_reward`. Make sure your agent does not execute more than 16 actions to solve the episode. If your learning process is efficient, the episode_total_reward should be at least 4.\n",
    "* Execute the last provided cell to visualize your agent's performance in navigating the environment effectively. Please note that it might take up to one minute to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d349c6c-7127-4917-a1e3-4b771ee666ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "print(f\"Number of states: {num_states}\")\n",
    "print(f\"Number of actions: {num_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c63e9-dacc-49ba-bebb-a5cecb57e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((num_states, num_actions)) # Global Q table\n",
    "\n",
    "# Getting the optimal policy\n",
    "def get_policy():\n",
    "    policy = {state: np.argmax(q_table[state]) for state in range(num_states)}\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740218f-7d0f-4890-8a33-6057d86ad8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning update\n",
    "def update_q_table(state, action, reward, new_state):\n",
    "    old_value = q_table[state, action]\n",
    "    next_max = max(q_table[new_state])\n",
    "    q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0855206-9451-4777-86cb-263172f7b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing epsilon_greedy()\n",
    "def epsilon_greedy(epsilon, state):\n",
    "    if np.random.rand() < epsilon: # Explore\n",
    "        action = env.action_space.sample()\n",
    "    else: # Exploit\n",
    "        action = np.argmax(q_table[state, :])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607c02f-66b3-470a-b981-a8e107b11fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning implementation\n",
    "episode_returns = []\n",
    "# Exploration\n",
    "epsilon = 1.0\n",
    "# Exploration rate\n",
    "epsilon_decay = 0.999\n",
    "min_epsilon = 0.01\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    actions_taken = 0\n",
    "    episode_reward = 0\n",
    "    while (not terminated) and (actions_taken < max_actions):\n",
    "        # # Random action selection\n",
    "        # action = env.action_space.sample()\n",
    "        # epsilon-greedy action\n",
    "        action = epsilon_greedy(epsilon, state)\n",
    "        # Take action and observe new state and reward\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        actions_taken += 1\n",
    "        # Update Q-table\n",
    "        update_q_table(state, action, reward, new_state)\n",
    "        episode_reward += reward\n",
    "        state = new_state\n",
    "    \n",
    "    episode_returns.append(episode_reward)\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff4ddb-3b97-47f6-bb2f-1aaeea8976bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the policy\n",
    "\n",
    "# Seed the environment for reproducibility\n",
    "env.np_random, _ = seeding.np_random(42)\n",
    "env.action_space.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "policy = get_policy()\n",
    "episode_total_reward = 0.0\n",
    "frames = []\n",
    "eval_max_actions = 16\n",
    "\n",
    "state, info = env.reset()\n",
    "terminated = False\n",
    "actions_taken = 0\n",
    "while (not terminated) and (actions_taken < eval_max_actions):\n",
    "    # Select the best action based on learned Q-table\n",
    "    action = policy[state]\n",
    "    # Take action and observe new state\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    actions_taken += 1\n",
    "    state = new_state\n",
    "    episode_total_reward += reward\n",
    "    frames.append(env.render())\n",
    "\n",
    "print(f\"Episode reward: {episode_total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf203e-4ef6-4399-8278-95453b4f818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('taxi_agent_behavior.gif', frames, fps=5, loop=0)\n",
    "\n",
    "# Display GIF\n",
    "gif_path = \"taxi_agent_behavior.gif\" \n",
    "Image(gif_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976c2a1-a80c-400b-ba72-2b69c7deffcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
