{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f64f8f-b4ba-4574-84e3-55f178e4728f",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Reinforcement Learning\n",
    "Examples use render mode `rgb_array`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c914be-744a-497d-9aab-3d9599613f6a",
   "metadata": {},
   "source": [
    "## Fundamentals of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f3303-2f1c-4def-804e-c521e38227cc",
   "metadata": {},
   "source": [
    "## Navigating the RL framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5ce23-8d19-433e-828d-16ad263fc2c1",
   "metadata": {},
   "source": [
    "#### Pseudo-code\n",
    "```\n",
    "# RL interaction loop\n",
    "env = create_environment()\n",
    "state = env.get_initial_state()\n",
    "for i in range(n_iterations):\n",
    "    action = choose_action(state)\n",
    "    state, reward = env.execute(action)\n",
    "    update_knowledge(state, action, reward)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72dcef-7d68-4248-8d49-e99f6bda7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical example\n",
    "import numpy as np\n",
    "expected_rewards = np.array([1, 6, 3])\n",
    "discount_factor = 0.9\n",
    "discounts = np.array([discount_factor ** i for i in range(len(expected_rewards))])\n",
    "print(f\"Discounts: {discounts}\")\n",
    "\n",
    "discounted_return = np.sum(expected_rewards * discounts)\n",
    "print(f\"The discounted return is {discounted_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1a921-235f-405e-936d-f3d43c2699a1",
   "metadata": {},
   "source": [
    "## Interacting with Gymnasium environments\n",
    "[CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e70dd-e40a-4c44-9362-a5482327040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and initializing the environment\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole', render_mode='rgb_array')\n",
    "state, info = env.reset(seed=42)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54df05-5bfc-4bec-b31a-fc3efd8bb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the state\n",
    "import matplotlib.pyplot as plt\n",
    "state_image = env.render()\n",
    "plt.imshow(state_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ddd0d-f92c-404c-be99-bce61eed15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the state\n",
    "import matplotlib.pyplot as plt\n",
    "def render():\n",
    "    state_image = env.render()\n",
    "    plt.imshow(state_image)\n",
    "    plt.show()\n",
    "# Call function\n",
    "render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9af4d-1baa-4528-81f2-c3b4b2a3229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "state, reward, terminated, _, _ = env.step(action)\n",
    "print(\"State: \", state)\n",
    "print(\"Reward: \", reward)\n",
    "print(\"Terminated: \", terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e9729-bd8c-426e-aaca-2a646e372e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction loops\n",
    "while not terminated:\n",
    "    action = 1 # Move to the right\n",
    "    state, reward, terminated, _, _ = env.step(action)\n",
    "    render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8975c8-f7b5-46cf-b985-2ba57da8d00d",
   "metadata": {},
   "source": [
    "# Chapter 2: Model-based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282053d1-a2ca-4eaf-91da-5ab372ab0e57",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7451c409-c1f6-49fa-bfa6-f101c1488ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n",
      "Number of actions:  4\n",
      "Number of states:  16\n"
     ]
    }
   ],
   "source": [
    "#Gymnasium states and actions\n",
    "import gymnasium as gym\n",
    "env = gym.make('FrozenLake', is_slippery=True)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Number of states: \", env.observation_space.n)\n",
    "# Discrete(4)\n",
    "# Discrete(16)\n",
    "# Number of actions: 4\n",
    "# Number of states: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec851fd1-674b-40e3-99c6-bf9d954a2bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "#Gymnasium rewards and transitions\n",
    "# env.unwrapped.P : dictionary where keys are state-action pairs\n",
    "# print(env.unwrapped.P[state][action])\n",
    "\n",
    "#Gymnasium rewards and transitions - example\n",
    "state = 6\n",
    "action = 0\n",
    "print(env.unwrapped.P[state][action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee5ea2-f08c-4887-a82e-6228e4dc35b8",
   "metadata": {},
   "source": [
    "## Policies and state-value functions\n",
    "This section seems to use a custom environment and so the code in this section can not be run without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e869b69-4d9e-47cb-b5a3-1386b0e377b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid world example: policy\n",
    "# 0: left, 1: down, 2: right, 3: up\n",
    "policy = {\n",
    "    0:1, 1:2, 2:1,\n",
    "    3:1, 4:3, 5:1,\n",
    "    6:2, 7:3\n",
    "}\n",
    "state, info = env.reset()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = policy[state]\n",
    "    state, reward, terminated, _, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5601a4c7-92c6-450c-81ad-e7ada3ae7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing state-values\n",
    "def compute_state_value(state):\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b744a9-efd1-43ca-bb40-3888bc62bed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m terminal_state = \u001b[32m8\u001b[39m\n\u001b[32m      3\u001b[39m gamma = \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m V = {state: compute_state_value(state) \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_states\u001b[49m)}\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(V)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#Changing policies\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 0: left, 1: down, 2: right, 3: up\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'num_states' is not defined"
     ]
    }
   ],
   "source": [
    "#Computing state-values\n",
    "terminal_state = 8\n",
    "gamma = 1\n",
    "V = {state: compute_state_value(state) for state in range(num_states)}\n",
    "print(V)\n",
    "\n",
    "#Changing policies\n",
    "# 0: left, 1: down, 2: right, 3: up\n",
    "policy_two = {\n",
    "    0:2, 1:2, 2:1,\n",
    "    3:2, 4:2, 5:1,\n",
    "    6:2, 7:2\n",
    "}\n",
    "V_2 = {state: compute_state_value(state) for state in range(num_states)}\n",
    "print(V_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444aaab-0901-4572-9d96-63bc5857a2db",
   "metadata": {},
   "source": [
    "## Action-value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c484a-510a-4de6-9cf6-7c7560b1adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Q-values\n",
    "def compute_q_value(state, action):\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5b164-fe20-4918-9e94-b69b86cb6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Q-values\n",
    "Q = {\n",
    "    (state, action): compute_q_value(state, action)\n",
    "    for state in range(num_states)\n",
    "    for action in range(num_actions)\n",
    "}\n",
    "print(Q)\n",
    "\n",
    "#Computing Q-values\n",
    "# {\n",
    "#     (0, 0): 0, (0, 1): 1, (0, 2): 7, (0, 3): 0,\n",
    "#     (1, 0): 0, (1, 1): 5, (1, 2): 8, (1, 3): 7,\n",
    "#     (2, 0): 7, (2, 1): 9, (2, 2): 8, (2, 3): 8,\n",
    "#     (3, 0): 1, (3, 1): 2, (3, 2): 5, (3, 3): 0,\n",
    "#     (4, 0): 1, (4, 1): 3, (4, 2): 9, (4, 3): 7,\n",
    "#     (5, 0): 5, (5, 1): 10, (5, 2): 9, (5, 3): 8,\n",
    "#     (6, 0): 2, (6, 1): 2, (6, 2): 3, (6, 3): 1,\n",
    "#     (7, 0): 2, (7, 1): 3, (7, 2): 10, (7, 3): 5,\n",
    "#     (8, 0): None, (8, 1): None, (8, 2): None, (8, 3): None\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb9b91-415e-40b3-951e-ea0f83680c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improving the policy\n",
    "improved_policy = {}\n",
    "for state in range(num_states-1):\n",
    "    max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "    improved_policy[state] = max_action\n",
    "print(improved_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063087f-583f-4bb9-a1b0-4ace1b83f31c",
   "metadata": {},
   "source": [
    "## Policy iteration and value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae99dac-d5c1-4743-8d05-a158b914e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid world\n",
    "policy = {\n",
    "    0:1, 1:2, 2:1,\n",
    "    3:1, 4:3, 5:1,\n",
    "    6:2, 7:3\n",
    "}\n",
    "\n",
    "#Policy evaluation\n",
    "def policy_evaluation(policy):\n",
    "    V = {state: compute_state_value(state, policy) for state in range(num_states)}\n",
    "    return V\n",
    "\n",
    "#Policy improvement\n",
    "def policy_improvement(policy):\n",
    "    improved_policy = {s: 0 for s in range(num_states-1)}\n",
    "    Q = {\n",
    "        (state, action): compute_q_value(state, action, policy)\n",
    "        for state in range(num_states) for action in range(num_actions)\n",
    "    }\n",
    "    for state in range(num_states-1):\n",
    "        max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "    return improved_policy\n",
    "\n",
    "#Policy iteration\n",
    "def policy_iteration():\n",
    "    policy = {0:1, 1:2, 2:1, 3:1, 4:3, 5:1, 6:2, 7:3}\n",
    "    while True:\n",
    "        V = policy_evaluation(policy)\n",
    "        improved_policy = policy_improvement(policy)\n",
    "        if improved_policy == policy:\n",
    "            break\n",
    "        policy = improved_policy\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c55a29-f133-46d8-b9c6-f09b8536bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal policy\n",
    "policy, V = policy_iteration()\n",
    "print(policy, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cd8ff-83c0-43b3-9a7c-01060136990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing value-iteration\n",
    "V = {state: 0 for state in range(num_states)}\n",
    "policy = {state:0 for state in range(num_states-1)}\n",
    "threshold = 0.001\n",
    "while True:\n",
    "    new_V = {state: 0 for state in range(num_states)}\n",
    "    for state in range(num_states-1):\n",
    "        max_action, max_q_value = get_max_action_and_value(state, V)\n",
    "        new_V[state] = max_q_value\n",
    "        policy[state] = max_action\n",
    "        if all(abs(new_V[state] - V[state]) < thresh for state in V):\n",
    "            break\n",
    "    V = new_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7a184-0e69-437b-a512-7cdc599707c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting optimal actions and values\n",
    "def get_max_action_and_value(state, V):\n",
    "    Q_values = [compute_q_value(state, action, V) for action in range(num_actions)]\n",
    "    max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "    max_q_value = Q_values[max_action]\n",
    "    return max_action, max_q_value\n",
    "\n",
    "#Computing Q-values\n",
    "def compute_q_value(state, action, V):\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    _, next_state, reward, _ = env.P[state][action][0]\n",
    "    return reward + gamma * V[next_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4725b84-0755-4756-ab56-d740a4ff8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal policy\n",
    "print(policy, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5816f1-3af2-463b-a514-9d36ce1a8233",
   "metadata": {},
   "source": [
    "# Chapter 3: Model-Free Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29094ae-018c-4d56-b0dc-1a880af77c6b",
   "metadata": {},
   "source": [
    "## Monte Carlo methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24321b-b203-4a72-9bb3-9c17fa75ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-free learning\n",
    "# Doesn't rely on knowledge of environment dynamics\n",
    "# Agent interacts with environment\n",
    "# Learns policy through trial and error\n",
    "# More suitable for real-world applications\n",
    "\n",
    "#Generating an episode\n",
    "def generate_episode():\n",
    "    episode = []\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9e485-e374-4b9c-9460-1bfa092abc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-visit Monte Carlo\n",
    "def first_visit_mc(num_episodes):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    for i in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        visited_states_actions = set()\n",
    "        for j, (state, action, reward) in enumerate(episode):\n",
    "            if (state, action) not in visited_states:\n",
    "                returns_sum[state, action] += sum([x[2] for x in episode[j:]])\n",
    "                returns_count[state, action] += 1\n",
    "                visited_states_actions.add((state, action))\n",
    "    nonzero_counts = returns_count != 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db7d8e8-1380-4a74-81db-c6c097cedfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# returns_sum = np.arange(32).reshape((8, 4))\n",
    "# returns_count = np.zeros((8, 4))\n",
    "# for coords in [(0, 1), (0, 3), (1, 2), (2, 3), (2, 1)]:\n",
    "#     returns_count[*coords] = 1\n",
    "# nonzero_counts = returns_count != 0\n",
    "# returns_sum[nonzero_counts] / returns_count[nonzero_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d85020-d945-4672-88cc-ac20b892ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every-visit Monte Carlo\n",
    "def every_visit_mc(num_episodes):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    for i in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        \n",
    "        for j, (state, action, reward) in enumerate(episode):\n",
    "            returns_sum[state, action] += sum([x[2] for x in episode[j:]])\n",
    "            returns_count[state, action] += 1\n",
    "\n",
    "    nonzero_counts = returns_count != 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f012873-4903-4346-b59e-8f7aefbeb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the optimal policy\n",
    "def get_policy():\n",
    "    policy = {state: np.argmax(Q[state]) for state in range(num_states)}\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565e85f-f0ca-4760-b835-44221feed2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting things together\n",
    "Q = first_visit_mc(1000)\n",
    "policy_first_visit = get_policy()\n",
    "print(\"First-visit policy: \\n\", policy_first_visit)\n",
    "\n",
    "Q = every_visit_mc(1000)\n",
    "policy_every_visit = get_policy()\n",
    "print(\"Every-visit policy: \\n\", policy_every_visit)\n",
    "\n",
    "# First-visit policy:\n",
    "# {0: 2, 1: 2, 2: 1,\n",
    "# 3: 2, 4: 2, 5: 0}\n",
    "# Every-visit policy:\n",
    "# {0: 2, 1: 2, 2: 1,\n",
    "# 3: 2, 4: 2, 5: 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536ad63-1da4-424b-a02b-6424d36db3e9",
   "metadata": {},
   "source": [
    "## Temporal difference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c057c4c5-42f7-420f-869a-5bd719e6b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "env = gym.make(\"FrozenLake\", is_slippery=False)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2c42d2b-af9a-4f45-8430-14f87aef4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA loop\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_action = env.action_space.sample()\n",
    "        update_q_table(state, action, reward, next_state, next_action)\n",
    "        state, action = next_state, next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d024f0b5-7105-4831-88a0-9065908189c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA updates\n",
    "def update_q_table(state, action, reward, next_state, next_action):\n",
    "    old_value = Q[state, action]\n",
    "    next_value = Q[next_state, next_action]\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "229f8eeb-ada9-464a-a38f-309cace59bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: np.int64(2), 1: np.int64(2), 2: np.int64(1), 3: np.int64(0), 4: np.int64(1), 5: np.int64(0), 6: np.int64(1), 7: np.int64(0), 8: np.int64(2), 9: np.int64(2), 10: np.int64(1), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(2), 15: np.int64(0)}\n"
     ]
    }
   ],
   "source": [
    "#Deriving the optimal policy\n",
    "policy = get_policy()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51f45c-dba8-4dca-9022-a55a04e2f875",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f09313d9-b5ab-4a28-9818-f2a9c391ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "# Q-learning implementation\n",
    "env = gym.make(\"FrozenLake\", is_slippery=True)\n",
    "num_episodes = 1000\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "num_states, num_actions = env.observation_space.n, env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "reward_per_random_episode = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf7eb614-5cc4-4d3a-9baa-8cc2065867df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning implementation\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        # Random action selection\n",
    "        action = env.action_space.sample()\n",
    "        # Take action and observe new state and reward\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Update Q-table\n",
    "        update_q_table(state, action, reward, new_state)\n",
    "        episode_reward += reward\n",
    "        state = new_state\n",
    "    \n",
    "    reward_per_random_episode.append(episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cc6428a-d3ec-4f60-8f70-3fd0c53bda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning update\n",
    "def update_q_table(state, action, reward, new_state):\n",
    "    old_value = Q[state, action]\n",
    "    next_max = max(Q[new_state])\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "261cb3ad-55fc-41e0-be71-9bc0c9d9e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the policy\n",
    "reward_per_learned_episode = []\n",
    "\n",
    "policy = get_policy()\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        # Select the best action based on learned Q-table\n",
    "        action = policy[state]\n",
    "        # Take action and observe new state\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "    reward_per_learned_episode.append(episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dfae63d-6d8b-4e53-b4a2-39c4e9f7b9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Q-learning evaluation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "avg_random_reward = np.mean(reward_per_random_episode)\n",
    "avg_learned_reward = np.mean(reward_per_learned_episode)\n",
    "plt.bar(\n",
    "    ['Random Policy', 'Learned Policy'],\n",
    "    [avg_random_reward, avg_learned_reward],\n",
    "    color=['blue', 'green']\n",
    ")\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e870d-2539-4eeb-b787-a73ff6ceafae",
   "metadata": {},
   "source": [
    "# Chapter 4: Advanced Stategies in Model-Free RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd7f58-e606-481f-890c-3f5417527dd4",
   "metadata": {},
   "source": [
    "## Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fa7ca8f-afd5-4bdd-ae9b-2d7fd622f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with Frozen Lake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4765d581-84ce-4cec-b56a-bd385cb98429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected SARSA update rule\n",
    "def update_q_table(state, action, next_state, reward):\n",
    "    expected_q = np.mean(Q[next_state])\n",
    "    Q[state, action] = (1-alpha) * Q[state, action] + alpha * (reward + gamma * expected_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3301c73-00f6-4e92-878f-37dbaec3479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for i in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        update_q_table(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "# Agent's policy\n",
    "policy = {state: np.argmax(Q[state]) for state in range(num_states)}\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9034e-9d2c-4228-ba24-76ed2eb7a046",
   "metadata": {},
   "source": [
    "## Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d627733-2138-43bd-bb86-f71f2d58f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with Frozen Lake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = [np.zeros((num_states, n_actions))] * 2\n",
    "num_episodes = 1000\n",
    "alpha = 0.5\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49204a37-af35-46b3-b756-82eaca52f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing update_q_tables()\n",
    "def update_q_tables(state, action, reward, next_state):\n",
    "    # Select a random Q-table index (0 or 1)\n",
    "    i = np.random.randint(2)\n",
    "    # Update the corresponding Q-table\n",
    "    best_next_action = np.argmax(Q[i][next_state])\n",
    "    Q[i][state, action] = (1 - alpha) * Q[i][state, action] + alpha * (reward + gamma * Q[1-i][next_state, best_next_action])\n",
    "\n",
    "# Training\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = np.random.choice(n_actions)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        update_q_tables(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "final_Q = (Q[0] + Q[1])/2\n",
    "# OR\n",
    "final_Q = Q[0] + Q[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d36e19-652c-41f6-b2aa-1512ca8f0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent's policy\n",
    "policy = {state: np.argmax(final_Q[state]) for state in range(num_states)}\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab153b9-ff96-4637-b133-1e58ea5e6228",
   "metadata": {},
   "source": [
    "## Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734426c-f947-4c6b-b5da-0ef614b07d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing exploration and exploitation\n",
    "\n",
    "# Implementation with Frozen Lake\n",
    "env = gym.make('FrozenLake', is_slippery=True)\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "Q = np.zeros((state_size, action_size))\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "total_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c14b93-79f3-4d24-87cd-327e9cd0206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing epsilon_greedy()\n",
    "def epsilon_greedy(state):\n",
    "    if np.random.rand() < epsilon: # Explore\n",
    "        action = env.action_space.sample()\n",
    "    else: # Exploit\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2e6a7-e76f-488d-90c2-e25b8ce52068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training epsilon-greedy\n",
    "epsilon = 0.9\n",
    "\n",
    "# Exploration rate\n",
    "rewards_eps_greedy = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        action = epsilon_greedy(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        Q[state, action] = update_q_table(state, action, new_state) # NOTE (JS): This is probably wrong, no need to return and reward is not passed as an argument\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "    rewards_eps_greedy.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c383e15-3ee5-4873-bcc0-7262af9919d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training decayed epsilon-greedy\n",
    "epsilon = 1.0\n",
    "\n",
    "# Exploration rate\n",
    "\n",
    "epsilon_decay = 0.999\n",
    "min_epsilon = 0.01\n",
    "rewards_decay_eps_greedy = []\n",
    "for episode in range(total_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        action = epsilon_greedy(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        Q[state, action] = update_q_table(state, action, new_state) # NOTE (JS): This is probably wrong, no need to return and reward is not passed as an argument\n",
    "        state = new_state\n",
    "    rewards_decay_eps_greedy.append(episode_reward)\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7f06d-580c-4600-bf99-aed7c849758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing strategies\n",
    "avg_eps_greedy= np.mean(rewards_eps_greedy)\n",
    "avg_decay = np.mean(rewards_decay_eps_greedy)\n",
    "plt.bar(['Epsilon Greedy', 'Decayed Epsilon Greedy'],\n",
    "[avg_eps_greedy, avg_decay],\n",
    "color=['blue', 'green'])\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436c1e4-c8b7-4a71-8e1e-068c34169ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi-armed bandits\n",
    "Gambler facing slot machines\n",
    "Challenge  maximize winning\n",
    "Solution  exploration-exploitation\n",
    "\n",
    "Slot machines\n",
    "\n",
    "Reward from an arm is 0 or 1\n",
    "Agent's goal  Accumulate maximum reward\n",
    "\n",
    "Solving the problem\n",
    "Decayed epsilon-greedy\n",
    "Epsilon  select random machine\n",
    "\n",
    "Solving the problem\n",
    "Decayed epsilon-greedy\n",
    "Epsilon  select random machine\n",
    "1 - epsilon  select best machine so far\n",
    "Epsilon decreases over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303124c9-8829-426b-b0f9-9a92f9e46486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "n_bandits = 4\n",
    "true_bandit_probs = np.random.rand(n_bandits)\n",
    "n_iterations = 100000\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.999\n",
    "counts = np.zeros(n_bandits)\n",
    "\n",
    "# How many times each bandit was played\n",
    "values = np.zeros(n_bandits)\n",
    "\n",
    "# Estimated winning probability of each bandit\n",
    "rewards = np.zeros(n_iterations)\n",
    "\n",
    "# Reward history\n",
    "selected_arms = np.zeros(n_iterations, dtype=int)\n",
    "\n",
    "# Arm selection history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7023a-9fe1-4cc1-bda2-26affcd43771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction loop\n",
    "for i in range(n_iterations):\n",
    "arm = epsilon_greedy()\n",
    "reward = np.random.rand() < true_bandit_probs[arm]\n",
    "rewards[i] = reward\n",
    "selected_arms[i] = arm\n",
    "counts[arm] += 1\n",
    "values[arm] += (reward - values[arm]) / counts[arm]\n",
    "epsilon = max(min_epsilon, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd47e6-a213-40ed-bdb8-2959b1aa988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing selections\n",
    "selections_percentage = np.zeros((n_iterations, n_bandits))\n",
    "\n",
    "# Analyzing selections\n",
    "selections_percentage = np.zeros((n_iterations, n_bandits))\n",
    "for i in range(n_iterations):\n",
    "selections_percentage[i, selected_arms[i]] = 1\n",
    "\n",
    "# Analyzing selections\n",
    "selections_percentage = np.zeros((n_iterations, n_bandits))\n",
    "for i in range(n_iterations):\n",
    "selections_percentage[i, selected_arms[i]] = 1\n",
    "selections_percentage = np.cumsum(selections_percentage, axis=0) /\n",
    "np.arange(1, n_iterations + 1).reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641174d-01d1-4168-aad9-63c686a12881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing selections\n",
    "for arm in range(n_bandits):\n",
    "plt.plot(selections_percentage[:, arm], label=f'Bandit #{arm+1}')\n",
    "plt.xscale('log')\n",
    "plt.title('Bandit Action Choices Over Time')\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Percentage of Bandit Selections (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "for i, prob in enumerate(true_bandit_probs, 1):\n",
    "print(f\"Bandit #{i} -> {prob:.2f}\")\n",
    "\n",
    "# Bandit #1 -> 0.37\n",
    "# Bandit #2 -> 0.95\n",
    "# Bandit #3 -> 0.73\n",
    "# Bandit #4 -> 0.60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
