{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f64f8f-b4ba-4574-84e3-55f178e4728f",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Reinforcement Learning\n",
    "Examples use render mode `rgb_array`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c914be-744a-497d-9aab-3d9599613f6a",
   "metadata": {},
   "source": [
    "## Fundamentals of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f3303-2f1c-4def-804e-c521e38227cc",
   "metadata": {},
   "source": [
    "## Navigating the RL framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5ce23-8d19-433e-828d-16ad263fc2c1",
   "metadata": {},
   "source": [
    "#### Pseudo-code\n",
    "```\n",
    "# RL interaction loop\n",
    "env = create_environment()\n",
    "state = env.get_initial_state()\n",
    "for i in range(n_iterations):\n",
    "    action = choose_action(state)\n",
    "    state, reward = env.execute(action)\n",
    "    update_knowledge(state, action, reward)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72dcef-7d68-4248-8d49-e99f6bda7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical example\n",
    "import numpy as np\n",
    "expected_rewards = np.array([1, 6, 3])\n",
    "discount_factor = 0.9\n",
    "discounts = np.array([discount_factor ** i for i in range(len(expected_rewards))])\n",
    "print(f\"Discounts: {discounts}\")\n",
    "\n",
    "discounted_return = np.sum(expected_rewards * discounts)\n",
    "print(f\"The discounted return is {discounted_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1a921-235f-405e-936d-f3d43c2699a1",
   "metadata": {},
   "source": [
    "## Interacting with Gymnasium environments\n",
    "[CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e70dd-e40a-4c44-9362-a5482327040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and initializing the environment\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole', render_mode='rgb_array')\n",
    "state, info = env.reset(seed=42)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54df05-5bfc-4bec-b31a-fc3efd8bb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the state\n",
    "import matplotlib.pyplot as plt\n",
    "state_image = env.render()\n",
    "plt.imshow(state_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ddd0d-f92c-404c-be99-bce61eed15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the state\n",
    "import matplotlib.pyplot as plt\n",
    "def render():\n",
    "    state_image = env.render()\n",
    "    plt.imshow(state_image)\n",
    "    plt.show()\n",
    "# Call function\n",
    "render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9af4d-1baa-4528-81f2-c3b4b2a3229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "state, reward, terminated, _, _ = env.step(action)\n",
    "print(\"State: \", state)\n",
    "print(\"Reward: \", reward)\n",
    "print(\"Terminated: \", terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e9729-bd8c-426e-aaca-2a646e372e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction loops\n",
    "while not terminated:\n",
    "    action = 1 # Move to the right\n",
    "    state, reward, terminated, _, _ = env.step(action)\n",
    "    render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8975c8-f7b5-46cf-b985-2ba57da8d00d",
   "metadata": {},
   "source": [
    "# Chapter 2: Model-based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282053d1-a2ca-4eaf-91da-5ab372ab0e57",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451c409-c1f6-49fa-bfa6-f101c1488ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gymnasium states and actions\n",
    "import gymnasium as gym\n",
    "env = gym.make('FrozenLake', is_slippery=True)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Number of states: \", env.observation_space.n)\n",
    "# Discrete(4)\n",
    "# Discrete(16)\n",
    "# Number of actions: 4\n",
    "# Number of states: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec851fd1-674b-40e3-99c6-bf9d954a2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gymnasium rewards and transitions\n",
    "# env.unwrapped.P : dictionary where keys are state-action pairs\n",
    "# print(env.unwrapped.P[state][action])\n",
    "\n",
    "#Gymnasium rewards and transitions - example\n",
    "state = 6\n",
    "action = 0\n",
    "print(env.unwrapped.P[state][action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee5ea2-f08c-4887-a82e-6228e4dc35b8",
   "metadata": {},
   "source": [
    "## Policies and state-value functions\n",
    "This section seems to use a custom environment and so the code in this section can not be run without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e869b69-4d9e-47cb-b5a3-1386b0e377b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid world example: policy\n",
    "# 0: left, 1: down, 2: right, 3: up\n",
    "policy = {\n",
    "    0:1, 1:2, 2:1,\n",
    "    3:1, 4:3, 5:1,\n",
    "    6:2, 7:3\n",
    "}\n",
    "state, info = env.reset()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = policy[state]\n",
    "    state, reward, terminated, _, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601a4c7-92c6-450c-81ad-e7ada3ae7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing state-values\n",
    "def compute_state_value(state):\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b744a9-efd1-43ca-bb40-3888bc62bed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Computing state-values\n",
    "terminal_state = 8\n",
    "gamma = 1\n",
    "V = {state: compute_state_value(state) for state in range(num_states)}\n",
    "print(V)\n",
    "\n",
    "#Changing policies\n",
    "# 0: left, 1: down, 2: right, 3: up\n",
    "policy_two = {\n",
    "    0:2, 1:2, 2:1,\n",
    "    3:2, 4:2, 5:1,\n",
    "    6:2, 7:2\n",
    "}\n",
    "V_2 = {state: compute_state_value(state) for state in range(num_states)}\n",
    "print(V_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444aaab-0901-4572-9d96-63bc5857a2db",
   "metadata": {},
   "source": [
    "## Action-value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c484a-510a-4de6-9cf6-7c7560b1adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Q-values\n",
    "def compute_q_value(state, action):\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5b164-fe20-4918-9e94-b69b86cb6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Q-values\n",
    "Q = {\n",
    "    (state, action): compute_q_value(state, action)\n",
    "    for state in range(num_states)\n",
    "    for action in range(num_actions)\n",
    "}\n",
    "print(Q)\n",
    "\n",
    "#Computing Q-values\n",
    "# {\n",
    "#     (0, 0): 0, (0, 1): 1, (0, 2): 7, (0, 3): 0,\n",
    "#     (1, 0): 0, (1, 1): 5, (1, 2): 8, (1, 3): 7,\n",
    "#     (2, 0): 7, (2, 1): 9, (2, 2): 8, (2, 3): 8,\n",
    "#     (3, 0): 1, (3, 1): 2, (3, 2): 5, (3, 3): 0,\n",
    "#     (4, 0): 1, (4, 1): 3, (4, 2): 9, (4, 3): 7,\n",
    "#     (5, 0): 5, (5, 1): 10, (5, 2): 9, (5, 3): 8,\n",
    "#     (6, 0): 2, (6, 1): 2, (6, 2): 3, (6, 3): 1,\n",
    "#     (7, 0): 2, (7, 1): 3, (7, 2): 10, (7, 3): 5,\n",
    "#     (8, 0): None, (8, 1): None, (8, 2): None, (8, 3): None\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb9b91-415e-40b3-951e-ea0f83680c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improving the policy\n",
    "improved_policy = {}\n",
    "for state in range(num_states-1):\n",
    "    max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "    improved_policy[state] = max_action\n",
    "print(improved_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063087f-583f-4bb9-a1b0-4ace1b83f31c",
   "metadata": {},
   "source": [
    "## Policy iteration and value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae99dac-d5c1-4743-8d05-a158b914e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid world\n",
    "policy = {\n",
    "    0:1, 1:2, 2:1,\n",
    "    3:1, 4:3, 5:1,\n",
    "    6:2, 7:3\n",
    "}\n",
    "\n",
    "#Policy evaluation\n",
    "def policy_evaluation(policy):\n",
    "    V = {state: compute_state_value(state, policy) for state in range(num_states)}\n",
    "    return V\n",
    "\n",
    "#Policy improvement\n",
    "def policy_improvement(policy):\n",
    "    improved_policy = {s: 0 for s in range(num_states-1)}\n",
    "    Q = {\n",
    "        (state, action): compute_q_value(state, action, policy)\n",
    "        for state in range(num_states) for action in range(num_actions)\n",
    "    }\n",
    "    for state in range(num_states-1):\n",
    "        max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "    return improved_policy\n",
    "\n",
    "#Policy iteration\n",
    "def policy_iteration():\n",
    "    policy = {0:1, 1:2, 2:1, 3:1, 4:3, 5:1, 6:2, 7:3}\n",
    "    while True:\n",
    "        V = policy_evaluation(policy)\n",
    "        improved_policy = policy_improvement(policy)\n",
    "        if improved_policy == policy:\n",
    "            break\n",
    "        policy = improved_policy\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c55a29-f133-46d8-b9c6-f09b8536bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal policy\n",
    "policy, V = policy_iteration()\n",
    "print(policy, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cd8ff-83c0-43b3-9a7c-01060136990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing value-iteration\n",
    "V = {state: 0 for state in range(num_states)}\n",
    "policy = {state:0 for state in range(num_states-1)}\n",
    "threshold = 0.001\n",
    "while True:\n",
    "    new_V = {state: 0 for state in range(num_states)}\n",
    "    for state in range(num_states-1):\n",
    "        max_action, max_q_value = get_max_action_and_value(state, V)\n",
    "        new_V[state] = max_q_value\n",
    "        policy[state] = max_action\n",
    "        if all(abs(new_V[state] - V[state]) < thresh for state in V):\n",
    "            break\n",
    "    V = new_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7a184-0e69-437b-a512-7cdc599707c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting optimal actions and values\n",
    "def get_max_action_and_value(state, V):\n",
    "    Q_values = [compute_q_value(state, action, V) for action in range(num_actions)]\n",
    "    max_action = max(range(num_actions), key=lambda a: Q_values[a])\n",
    "    max_q_value = Q_values[max_action]\n",
    "    return max_action, max_q_value\n",
    "\n",
    "#Computing Q-values\n",
    "def compute_q_value(state, action, V):\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    _, next_state, reward, _ = env.P[state][action][0]\n",
    "    return reward + gamma * V[next_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4725b84-0755-4756-ab56-d740a4ff8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal policy\n",
    "print(policy, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5816f1-3af2-463b-a514-9d36ce1a8233",
   "metadata": {},
   "source": [
    "# Chapter 3: Model-Free Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29094ae-018c-4d56-b0dc-1a880af77c6b",
   "metadata": {},
   "source": [
    "## Monte Carlo methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24321b-b203-4a72-9bb3-9c17fa75ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-free learning\n",
    "# Doesn't rely on knowledge of environment dynamics\n",
    "# Agent interacts with environment\n",
    "# Learns policy through trial and error\n",
    "# More suitable for real-world applications\n",
    "\n",
    "#Generating an episode\n",
    "def generate_episode():\n",
    "    episode = []\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9e485-e374-4b9c-9460-1bfa092abc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-visit Monte Carlo\n",
    "def first_visit_mc(num_episodes):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    for i in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        visited_states_actions = set()\n",
    "        for j, (state, action, reward) in enumerate(episode):\n",
    "            if (state, action) not in visited_states:\n",
    "                returns_sum[state, action] += sum([x[2] for x in episode[j:]])\n",
    "                returns_count[state, action] += 1\n",
    "                visited_states_actions.add((state, action))\n",
    "    nonzero_counts = returns_count != 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7d8e8-1380-4a74-81db-c6c097cedfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# returns_sum = np.arange(32).reshape((8, 4))\n",
    "# returns_count = np.zeros((8, 4))\n",
    "# for coords in [(0, 1), (0, 3), (1, 2), (2, 3), (2, 1)]:\n",
    "#     returns_count[*coords] = 1\n",
    "# nonzero_counts = returns_count != 0\n",
    "# returns_sum[nonzero_counts] / returns_count[nonzero_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d85020-d945-4672-88cc-ac20b892ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every-visit Monte Carlo\n",
    "def every_visit_mc(num_episodes):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    returns_sum = np.zeros((num_states, num_actions))\n",
    "    returns_count = np.zeros((num_states, num_actions))\n",
    "    for i in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        \n",
    "        for j, (state, action, reward) in enumerate(episode):\n",
    "            returns_sum[state, action] += sum([x[2] for x in episode[j:]])\n",
    "            returns_count[state, action] += 1\n",
    "\n",
    "    nonzero_counts = returns_count != 0\n",
    "    Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f012873-4903-4346-b59e-8f7aefbeb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the optimal policy\n",
    "def get_policy():\n",
    "    policy = {state: np.argmax(Q[state]) for state in range(num_states)}\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565e85f-f0ca-4760-b835-44221feed2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting things together\n",
    "Q = first_visit_mc(1000)\n",
    "policy_first_visit = get_policy()\n",
    "print(\"First-visit policy: \\n\", policy_first_visit)\n",
    "\n",
    "Q = every_visit_mc(1000)\n",
    "policy_every_visit = get_policy()\n",
    "print(\"Every-visit policy: \\n\", policy_every_visit)\n",
    "\n",
    "# First-visit policy:\n",
    "# {0: 2, 1: 2, 2: 1,\n",
    "# 3: 2, 4: 2, 5: 0}\n",
    "# Every-visit policy:\n",
    "# {0: 2, 1: 2, 2: 1,\n",
    "# 3: 2, 4: 2, 5: 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536ad63-1da4-424b-a02b-6424d36db3e9",
   "metadata": {},
   "source": [
    "## Temporal difference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057c4c5-42f7-420f-869a-5bd719e6b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "env = gym.make(\"FrozenLake\", is_slippery=False)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c42d2b-af9a-4f45-8430-14f87aef4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA loop\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_action = env.action_space.sample()\n",
    "        update_q_table(state, action, reward, next_state, next_action)\n",
    "        state, action = next_state, next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024f0b5-7105-4831-88a0-9065908189c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA updates\n",
    "def update_q_table(state, action, reward, next_state, next_action):\n",
    "    old_value = Q[state, action]\n",
    "    next_value = Q[next_state, next_action]\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f8eeb-ada9-464a-a38f-309cace59bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deriving the optimal policy\n",
    "policy = get_policy()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51f45c-dba8-4dca-9022-a55a04e2f875",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09313d9-b5ab-4a28-9818-f2a9c391ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "# Q-learning implementation\n",
    "env = gym.make(\"FrozenLake\", is_slippery=True)\n",
    "num_episodes = 1000\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "num_states, num_actions = env.observation_space.n, env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "reward_per_random_episode = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7eb614-5cc4-4d3a-9baa-8cc2065867df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning implementation\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        # Random action selection\n",
    "        action = env.action_space.sample()\n",
    "        # Take action and observe new state and reward\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Update Q-table\n",
    "        update_q_table(state, action, reward, new_state)\n",
    "        episode_reward += reward\n",
    "        state = new_state\n",
    "    \n",
    "    reward_per_random_episode.append(episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6428a-d3ec-4f60-8f70-3fd0c53bda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning update\n",
    "def update_q_table(state, action, reward, new_state):\n",
    "    old_value = Q[state, action]\n",
    "    next_max = max(Q[new_state])\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cb3ad-55fc-41e0-be71-9bc0c9d9e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the policy\n",
    "reward_per_learned_episode = []\n",
    "\n",
    "policy = get_policy()\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        # Select the best action based on learned Q-table\n",
    "        action = policy[state]\n",
    "        # Take action and observe new state\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "    reward_per_learned_episode.append(episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfae63d-6d8b-4e53-b4a2-39c4e9f7b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning evaluation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "avg_random_reward = np.mean(reward_per_random_episode)\n",
    "avg_learned_reward = np.mean(reward_per_learned_episode)\n",
    "plt.bar(\n",
    "    ['Random Policy', 'Learned Policy'],\n",
    "    [avg_random_reward, avg_learned_reward],\n",
    "    color=['blue', 'green']\n",
    ")\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e870d-2539-4eeb-b787-a73ff6ceafae",
   "metadata": {},
   "source": [
    "# Chapter 4: Advanced Stategies in Model-Free RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd7f58-e606-481f-890c-3f5417527dd4",
   "metadata": {},
   "source": [
    "## Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7ca8f-afd5-4bdd-ae9b-2d7fd622f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with Frozen Lake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765d581-84ce-4cec-b56a-bd385cb98429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected SARSA update rule\n",
    "def update_q_table(state, action, next_state, reward):\n",
    "    expected_q = np.mean(Q[next_state])\n",
    "    Q[state, action] = (1-alpha) * Q[state, action] + alpha * (reward + gamma * expected_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3301c73-00f6-4e92-878f-37dbaec3479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for i in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        update_q_table(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "# Agent's policy\n",
    "policy = {state: np.argmax(Q[state]) for state in range(num_states)}\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9034e-9d2c-4228-ba24-76ed2eb7a046",
   "metadata": {},
   "source": [
    "## Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d627733-2138-43bd-bb86-f71f2d58f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with Frozen Lake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "num_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = [np.zeros((num_states, n_actions))] * 2\n",
    "num_episodes = 1000\n",
    "alpha = 0.5\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49204a37-af35-46b3-b756-82eaca52f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing update_q_tables()\n",
    "def update_q_tables(state, action, reward, next_state):\n",
    "    # Select a random Q-table index (0 or 1)\n",
    "    i = np.random.randint(2)\n",
    "    # Update the corresponding Q-table\n",
    "    best_next_action = np.argmax(Q[i][next_state])\n",
    "    Q[i][state, action] = (1 - alpha) * Q[i][state, action] + alpha * (reward + gamma * Q[1-i][next_state, best_next_action])\n",
    "\n",
    "# Training\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = np.random.choice(n_actions)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        update_q_tables(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "final_Q = (Q[0] + Q[1])/2\n",
    "# OR\n",
    "final_Q = Q[0] + Q[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d36e19-652c-41f6-b2aa-1512ca8f0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent's policy\n",
    "policy = {state: np.argmax(final_Q[state]) for state in range(num_states)}\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab153b9-ff96-4637-b133-1e58ea5e6228",
   "metadata": {},
   "source": [
    "## Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734426c-f947-4c6b-b5da-0ef614b07d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing exploration and exploitation\n",
    "\n",
    "# Implementation with Frozen Lake\n",
    "env = gym.make('FrozenLake', is_slippery=True)\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "Q = np.zeros((state_size, action_size))\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "total_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c14b93-79f3-4d24-87cd-327e9cd0206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing epsilon_greedy()\n",
    "def epsilon_greedy(state):\n",
    "    if np.random.rand() < epsilon: # Explore\n",
    "        action = env.action_space.sample()\n",
    "    else: # Exploit\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2e6a7-e76f-488d-90c2-e25b8ce52068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training epsilon-greedy\n",
    "epsilon = 0.9\n",
    "\n",
    "# Exploration rate\n",
    "rewards_eps_greedy = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        action = epsilon_greedy(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        Q[state, action] = update_q_table(state, action, new_state) # NOTE (JS): This is probably wrong, no need to return and reward is not passed as an argument\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "    rewards_eps_greedy.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c383e15-3ee5-4873-bcc0-7262af9919d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training decayed epsilon-greedy\n",
    "epsilon = 1.0\n",
    "\n",
    "# Exploration rate\n",
    "\n",
    "epsilon_decay = 0.999\n",
    "min_epsilon = 0.01\n",
    "rewards_decay_eps_greedy = []\n",
    "for episode in range(total_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        action = epsilon_greedy(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        Q[state, action] = update_q_table(state, action, new_state) # NOTE (JS): This is probably wrong, no need to return and reward is not passed as an argument\n",
    "        state = new_state\n",
    "    rewards_decay_eps_greedy.append(episode_reward)\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7f06d-580c-4600-bf99-aed7c849758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing strategies\n",
    "avg_eps_greedy= np.mean(rewards_eps_greedy)\n",
    "avg_decay = np.mean(rewards_decay_eps_greedy)\n",
    "plt.bar(['Epsilon Greedy', 'Decayed Epsilon Greedy'],\n",
    "[avg_eps_greedy, avg_decay],\n",
    "color=['blue', 'green'])\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecdb3cd-9abe-4bca-8258-7e0c9ae1e504",
   "metadata": {},
   "source": [
    "## Multi-armed bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303124c9-8829-426b-b0f9-9a92f9e46486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "n_bandits = 4\n",
    "true_bandit_probs = np.random.rand(n_bandits)\n",
    "\n",
    "n_iterations = 100000\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "counts = np.zeros(n_bandits) # How many times each bandit was played\n",
    "values = np.zeros(n_bandits) # Estimated winning probability of each bandit\n",
    "rewards = np.zeros(n_iterations) # Reward history\n",
    "selected_arms = np.zeros(n_iterations, dtype=int) # Arm selection history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff9db9-7587-4c3f-b3a3-cdb47c6803f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing epsilon_greedy() for multi-armed bandits\n",
    "def epsilon_greedy():\n",
    "    if np.random.rand() < epsilon: # Explore\n",
    "        action = env.action_space.sample()\n",
    "    else: # Exploit\n",
    "        action = np.argmax(values)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7023a-9fe1-4cc1-bda2-26affcd43771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction loop\n",
    "for i in range(n_iterations):\n",
    "    arm = epsilon_greedy()\n",
    "    reward = np.random.rand() < true_bandit_probs[arm]\n",
    "    rewards[i] = reward\n",
    "    selected_arms[i] = arm\n",
    "    counts[arm] += 1\n",
    "    values[arm] += (reward - values[arm]) / counts[arm]\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd47e6-a213-40ed-bdb8-2959b1aa988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing selections\n",
    "selections_percentage = np.zeros((n_iterations, n_bandits))\n",
    "for i in range(n_iterations):\n",
    "    if (i % 10000 == 0):\n",
    "        print(f\"Iteration {i}\")\n",
    "    selections_percentage[i, selected_arms[i]] = 1\n",
    "selections_percentage = np.cumsum(selections_percentage, axis=0) / np.arange(1, n_iterations + 1).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641174d-01d1-4168-aad9-63c686a12881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing selections\n",
    "for arm in range(n_bandits):\n",
    "    plt.plot(selections_percentage[:, arm], label=f'Bandit #{arm+1}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title('Bandit Action Choices Over Time')\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Percentage of Bandit Selections (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "for i, prob in enumerate(true_bandit_probs, 1):\n",
    "    print(f\"Bandit #{i} -> {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667497e-5233-4e12-acd9-bfe404c49679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
